# Ideas and relevant questions for RL research.
The first sections are half-good ideas that might fit somewhere, so they deserved their own section to dig into.
The last section is just a messy bundle of random thoughts and questions.

## World Model Dream Sampling
With Kevin. It seems in the WM paper they use initial game frames to start the dream, when learning full dreamy. However, according to Kevin, a VAE should be able to sample initial dream data only by sampling from the compressed information vector. If this is true, the dream could start anywhere in the world (not only initial distribution), therefore simulating *random starts*. Not only can we rid now ourselves completely of the real-world data, we actually might benefit from it! (with unusual data, preventing overfitting and -a bit- correlated data). It (still) can be argued that random starts might not be a very essential idea since we might sample useless states, that would never show up in the optimal policy. However, if the VAE is trained along with the agent, it should compress mainly experienced data, therefore mostly useful data; hence not completely "random starts" (as it doesn't correspond to any state in the environment), but *semi-random starts* with more weight to interesting states.

## Ideas to dig into
* Ability to **generalize to open problems**: formulation of sub-goals that can restrict the span of possible actions and behaviors?

* **Hint the mechanisms of the environment in the agent architecture** - the brain doesn't start from scratch all the time; we are wired to solve physics problems and reality-consistent situations and do poorly with absurd environments. How can we stick these into the agent's fundamental wirings to help it? examples: CNN takes into account properties of images; HyperNEAT (and forgot the name of the nets there) build onto the assumption that the sensory world is often symmetrical and uses it to evolve architectures accordingly. Relational nets, also; although this is `TOREAD`
* **Highly deterministic environments**: most real-world (especially physical) problems have a very strong deterministic component; only some states (or time steps) have a stochastic component to them, while most other transitions and rewards are very consistent and easily predicted. This means that building a model of the environment might not, in these cases, be such a hard problem; and the distributional model we can learn from it might not be that complex, only branching out to multiple possibilities in specific (or just, few) cases. In planning, we can then build trajectories with some measure of confidence that they might occur (be sampled). This observation doesn't seem to impact in any way the algorithms&methods that might stem from the usual formalization; but it hints that it might be a practical possibility, while most models are just sample-based due to the assumption that distributional models are too complex to be approximated.

* **Off Policy planning and learning**: they raise multiple problems of off-policy learning such as high variance or the simple fact that it doesn't really work well with function approximation. The main cause seems to be the very nature of the method; i.e. that the data observed isn't the one of the policy we want to learn, so the learning process is tedious. However we really want to be able to have off-policy methods; first because we might not have absolute control over the data input (noisy actions, or learning by observing an expert), but also because we might want to learn multiple policies at once - various strategies to tackle the same problem. This joins what was said in the planning part: you sometimes want to consider a completely different behavior, rather than the "one-step" foreseeing of the Q value function; you might want to compare complete policies. This calls either for some kind of control over the environment, where we would have to rewind back up to compare policies; or for off-policy methods where we still use the data from just one but learn multiple. To get the best out of both off and on-policy methods, we could consider learning off-policy in planning space: we learn a model thanks to the behavior policy, and using it, learn about the off-policy but *on-policy* within a "dreamt" simulation. The main difference with simple look-ahead planning to find the best move or policy is that we could actually *learn* the off-policy in our simulation, not just use the outcome to make a decision (MCTS way, where everything is discarded at the end. Even AlphaZero discards the explored experiences)

* **Learning more from an MCTS search** - as we just said AlphaZero discards explored branches of the tree, in the sense that learning only occurs on the main (true-environment) experience. In that sense, it isn't a "complete" model-based algorithm as it nearly doesn't use its perfect model to learn, only to plan. From an MCTS search, the only thing we learn is the resulting search policy \pi. Is there a way to *learn more* from that search? Ideally, in a Meta setting, we would be able to understand the general guidelines of "learning" (since we have a base policy and an improved policy) to apply this learning process to future steps or environments altogether. From a practical point of view, we could learn the \pi policies of all the explored nodes of the tree, but give a weaker weight (e.g. exponentially decreasing weight with depth) to the updates.

* Can you train **MuZero with a GA**? Since it was trained end-to-end. You would still need the network to output value functions, in order to guide MCTS, which can be a major disadvantage - but could also not be: even if the values do not correspond to actual value functions, it could be useful. The GA could even come up with its own rewarding process in planning to boost some behaviors..? Sounds fun.

* **Importance sampling** seems soooo **random & arbitrary**. Why just divide the policies? More legit seems to be to take the conditional probability to end up in this position with policy pi given that you got there with policy b...

* Isn't a bit stupid that the agent should **understand the reward** by itself? I mean, if we were to play Gridworld, we would know which is the terminal state; doesn't really make sense (for real world problems) to have an environment where the goal is unknown before we start. Would seem more legit to *explain* the reward (goal) - one naive way to do it would be to backup a few times from the transitions that provide reward, so the agent knows a bit how to behave if it is close to a rewardy transition... Obviously, this doesn't apply to the general case and implies control and prior knowledge over the environment; but on many problems the goal is actually pretty easily "explainable"; the way to get there is not...? I guess for humans, explaining the goal allows us to plan back to how we could achieve it; so for an agent, it kind of comes back to curriculum learning. Could we motivate *self-designed curriculum learning* based on *goal explanation* ?

* I'm surprised **Eligibility traces** aren't used much more than they seem; such a powerful tool! `TOREAD` the True Online TD(lambda) paper.

* Basically what evolution did was **evolve different rewarding signals** for the brain. The brain that attributed reward/punishment signals to the stimulus that resulted in survival were selected. Also obviously, the brain architecture itself was evolved; but it's interesting to think that we could try to run a simulation of animals and only evolve their reward structures, with one-hot survival as the fitness function. So more formally, the population would be reward functions (eg weights of a parameterized function as DNA) and the policy would be the resulting behavior of an RL agent when trained with said reward function. Ideally, obviously, we would also evolve different agent architectures, algorithms, etc. so the whole brain & body process. The here proposed algorithm, clearly, doesn't solve the sparse reward setting as the agents would never see the reward in the first place, except randomly (there is no direction to follow), so the GA is powerless. But what if we add a *curiosity* component to our fitness? Like a motivation to behave differently, explore the state-space, or whatever measure... And have it reduced over the number of successful goal reached? Anyway as Matthias mentioned, GA+RL is exponentially data-inefficient. Still interesting to see the resulting behaviors though. But wouldn't the evolved reward function actually simply learn the value function? Then the agent could learn to just follow reward greedily .^.

* Learning **Auxiliary tasks and goals** sharing weights with the rest of the network (or more generally, same agent until some point). The auxiliary tasks can be prediction of something else (physics, distribution, whatever other than the goal value function or policy); this will allow the network to have a wider generalization span over possible behaviors, as introduced in the book. they say it usually leads to faster training. The main question is: *what* auxiliary tasks, *how* to train them, and *why* (higher level question to understand what led to results)? For example, we might simply want to use some kind of future-predicting NN head with a shared body of the rest of the network; the weights learnt to do that could help understanding the environment on a more fundamental level and help the policy or value evaluation. (broadly speaking, I put that there because I like this idea and it kinda links to options)

* Are **Options really the way to go ?** What options try to solve is how we can go forward in time many time steps at once, with a predefined behavior. They achieve this simply by using an auxiliary, controlling policy for a short time; enabling the meta agent to consider the environment on a faster time scale. However, it feels a bit more intuitive to talk about sub-goal states; however we might reach them..? This is fundamentally the same thing, but the options don't intrinsically carry information about what state I should end up in; they are just a temporary policy - although you just set gamma(S) = 1_{S=S_target} to reach this result, I guess. Anyway this was motivated by the observation that when humans plan ahead, they do not plan over their action space directly, they just plan in approximate states they will end up in - "I'm going to take the elevator, and go to the bakery to buy a sandwich" - but in the end we are able to plan in future states space because we know what our abilities are, hence what our policy ultimately will yield. Guess it is hard (and not necessarily good practice) to compare with or get inspired by human ways of functioning, since we barely know how our brain computes all of this internally; we just experience the illusion of consciousness and choice that arises from the process. This process occurs at a different scale than atomic actions because the brain disregarded most details to make it easier to process and choose from, but this is still our consciousness' observation of the partially observable thoughts of the brain. Hence, this whole thread probably leads nowhere; but dead ends are ends!

* Can we somehow **boost the motivation for learning** by giving intrinsic reward when the agent learns something? So the agent would become knowledge-greedy and start striving for new things. Very, very hard to formalize, but the idea surely yields promising results and even interesting Meta-Learning properties. In a relational setting, this could be implemented as giving intrinsic reward when new connections are made between objects, or when such connections are confirmed by experience. The problem is of credit assignment, in rewarding the *learning process* as opposed to the learnt target. This also comes down to knowledge representation: what is *learning*? We could simply give reward to whatever actions lead to TD errors; but this would favor bad actions in punishment settings -just limit it to positive errors then-. A possible way to entail that seems to be on the HRL side, where the Meta would get reward when his subpolicies learn stuff. Doesn't solve at all any of the problems though.

* If **not Deep Learning, then what?** Looking for an alternative solution to deep; however they offer such interesting learning properties of being non-linear and universal approximators. However, they come with lots of problems like catastrophic inference, slow learning with astronomic amounts of data - and therefore not being fit for online training.

* In a HRL setting, what if one of the **options is simply "explore"**? For example the meta-controller gives its orders and one of them is simply to explore the environment; the agent then acts w.r.t the curious intrinsic reward. Other orders lead to usual reward distributions e.g. one-hot goal-achievement. The advantage of that is then that we have developed an auxiliary task that the controller will learn to master, and in the process, learn much about the environment's dynamics, therefore being stronger when needing to solve other tasks. This kills two birds with one stone (let's say instead, feeds two bird with one seed) because we have a curious algorithm and policy on demand, but we also learn about the general task through auxiliary learning. Ideally, this would blend smoothly in the rest of the meta-slave vocabulary and language, otherwise we'd have to design a special keyword (meta-output -> controller-input) to ask for exploration. This comes back to the question of the meta/slave language, raising the point that just giving (latent) states as goals is not enough to sum up everything the meta could like to convey to the slave. See the dedicated .md file.

* How can we create a good **Artificial Curiosity**? We can try to motivate the learning process itself, although it can be  difficult to measure. The book mentions "unexpected, novel, or otherwise interesting input, or can assess the agentâ€™s ability to cause changes in its environment" as possible triggers of intrinsic reward. So unexpected and novel is kind of the prediction approach (or "surprise") to curiosity & exploration; can there be other approaches for the same idea? "Otherwise interesting input" is hard to formalize but might refer to newly observed interactions, relationships (hence relational, I think). "The agent's ability to cause changes in the environment" is an interesting point and the obvious answer to the Noisy TV problem: the agent isn't *doing* anything that could get it to deserve reward: we need a measure of *credit* of the intrinsic reward with respect to the behavior policy. How can we formalize this? Looking at the higher level, *Is there a way to motivate behavior beyond reward "faking"?* As in, beyond intrinsic reward, which might be slow, or not experienced/too hard to find, rewinding back to the sparse reward problem - or useless, arbitrary reward assignment problem that leads to no interesting exploratory behavior. Alternatives might be learning a value function directly for a state we think is interesting, so the backups will be done faster (still pretty much the same idea). Supervised learning of exploratory behaviors? (as a more direct method -- at the policy level). We could also consider reward signal learning by the agent, in order to motivate exploratory behavior - but how do we *judge* the quality of the reward then, so the agent knows what to reward? Instead of motivating curiosity, how about **punishing laziness**? As in, a sort of *boredom measure* where the agent is expelled from known territory by negative reward. This is basically the same idea, but might be easier to formulate and implement than to try and give a measure of *novelty* and *originality*.

* **Model-based learning** with some kind of consistent **confidence measure**? so the model could tell how confident it is about a state, or a transition. The point is then that below an arbitrary confidence threshold, there is no point in going any further, since the model will suck anyway. The good thing about inserting such a confidence idea at the *transition* level is then that the model could be confident about some transitions, but not about others, so we can have a consistent trajectory until some new event or situation. This thing could be trained on the L2 loss of the model..? This would yield some kind of *self-aware* model that doesn't take the dreaming agent to irrelevant states. This confidence measure could also then be used by some curiosity reward shaping to motivate the agent to reach new states, therefore also teaching new things to the model.
